<!DOCTYPE html PUBLIC "-//W3C//DTD Html 1.0 Strict//EN" "http://www.w3.org/TR/Html1/DTD/strict.dtd">
<html><head>
<title>Tricks</title>

<link crossorigin="anonymous" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" rel="stylesheet"/>
<script crossorigin="anonymous" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" src="https://code.jquery.com/jquery-3.3.1.slim.min.js"></script>
<script crossorigin="anonymous" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js"></script>
<script crossorigin="anonymous" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js"></script>
<link href="/css/bootstrap.css" rel="stylesheet"/>
<link href="images/favicon.ico" rel="icon" type="image/x-icon"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
</head>
<body>
<nav class="navbar navbar-expand sticky-top navbar-light bg-light">
<a class="navbar-brand" href="/"><img src="/images/logo.svg"/>MultiBUGS</a>
<div class="collapse navbar" id="navbarSupportedContent">
<ul class="navbar-nav mr-auto">
<li class="nav-item">
<a class="nav-link" href="/">Home</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/about">About</a>
</li>
<li class="nav-item">
<a class="nav-link" href="/download">Download</a>
</li>
<li class="nav-item active">
<a class="nav-link" href="/documentation/latest/">Documentation <span class="sr-only">(current)</span></a>
</li>
<li class="nav-item">
<a class="nav-link" href="/examples/latest/">Examples</a>
</li>
</ul>
</div>
</nav><div class="container-fluid"><div class="row main"><div class="col-md-3 col-lg-2 col-xl-3"><div class="sticky-top stickymenu" id="contents">
<button aria-controls="navbarContents" aria-expanded="false" aria-label="Toggle docs navigation" class="btn btn-link d-md-none ml-3 mr-3 mt-3 collapsed mb-toggler" data-target="#navbarContents" data-toggle="collapse" type="button">
<span class="mb-toggler-icon mr-1"></span>
    Contents
  </button>

<h5 class="d-none d-md-block">Contents</h5>
<div class="collapse mb-links" id="navbarContents">
<ul class="list-unstyled">
<li class="nav-item"><a aria-expanded="true" class="dropdown-toggle nav-link" data-toggle="collapse" href="#basics">Basics</a>
<ul class="collapse" id="basics">
<li><a href="/documentation/latest/Introduction.html">Introduction</a></li>
<li><a href="/documentation/latest/Tutorial.html">Tutorial</a></li>
<li><a href="/documentation/latest/CompoundDocuments.html">Compound documents</a></li>
<li><a href="/documentation/latest/Graphics.html">Graphics</a></li>
<li><a href="/documentation/latest/DoodleBUGS.html">DoodleBUGS</a></li>
<li><a href="/documentation/latest/Scripts.html">Scripts</a></li>
</ul>
</li>
<li class="nav-item"><a aria-expanded="true" class="dropdown-toggle nav-link" data-toggle="collapse" href="#language">BUGS language</a>
<ul class="collapse show" id="language">
<li><a href="/documentation/latest/ModelSpecification.html">Model specification</a></li>
<li><a href="/documentation/latest/Distributions.html">Distributions</a></li>
<li><a href="/documentation/latest/Functions.html">Functions and functionals</a></li>
<li>Advanced uses</li>
<li><a href="/documentation/latest/TipsTroubleshooting.html">Tips and troubleshooting</a></li>
</ul>
</li>
<li class="nav-item"><a aria-expanded="true" class="dropdown-toggle nav-link" data-toggle="collapse" href="#menus">Menus</a>
<ul class="collapse" id="menus">
<li><a href="/documentation/latest/FileMenu.html">File</a></li>
<li><a href="/documentation/latest/EditMenu.html">Edit</a></li>
<li><a href="/documentation/latest/AttributesMenu.html">Attributes</a></li>
<li><a href="/documentation/latest/ToolsMenu.html">Tools</a></li>
<li><a href="/documentation/latest/TextMenu.html">Text</a></li>
<li><a href="/documentation/latest/InfoMenu.html">Info</a></li>
<li><a href="/documentation/latest/ModelMenu.html">Model</a></li>
<li><a href="/documentation/latest/InferenceMenu.html">Inference</a></li>
<li><a href="/documentation/latest/DoodleMenu.html">Doodle</a></li>
<li><a href="/documentation/latest/MapMenu.html">Map</a></li>
<li><a href="/documentation/latest/ExamplesMenu.html">Examples</a></li>
<li><a href="/documentation/latest/ManualsMenu.html">Manuals</a></li>
<li><a href="/documentation/latest/HelpMenu.html">Help</a></li>
</ul>
</li>
<li class="nav-item"><a aria-expanded="true" class="dropdown-toggle nav-link" data-toggle="collapse" href="#geobugs">GeoBUGS</a>
<ul class="collapse" id="geobugs">
<li><a href="/documentation/latest/spatial/Manual.html">Introduction</a></li>
<li><a href="/documentation/latest/spatial/ImportExport.html">Importing/exporting maps</a></li>
<li><a href="/documentation/latest/spatial/AdjacencyMatrices.html">Adjacency matrices</a></li>
<li><a href="/documentation/latest/spatial/Maps.html">Creating maps</a></li>
<li><a href="/documentation/latest/spatial/SpatialDistributions.html">Spatial distributions</a></li>
<li><a href="/documentation/latest/spatial/TemporalDistributions.html">Temporal distributions</a></li>
</ul>
</li>
<li class="nav-item"><a aria-expanded="true" class="dropdown-toggle nav-link" data-toggle="collapse" href="#reliability">ReliaBUGS</a>
<ul class="collapse" id="reliability">
<li><a href="/documentation/latest/reliability/Contents.html">Introduction</a></li>
<li><a href="/documentation/latest/reliability/Analysis.html">Basics</a></li>
<li><a href="/documentation/latest/reliability/Distributions.html">Distributions</a></li>
<li><a href="/documentation/latest/reliability/Functions.html">Logical functions</a></li>
<li><a href="/documentation/latest/reliability/References.html">References</a></li>
</ul>
</li>
<li class="nav-item"><a aria-expanded="true" class="dropdown-toggle nav-link" data-toggle="collapse" href="#developer">Developer</a>
<ul class="collapse" id="developer">
<li><a href="/documentation/latest/developer/Manual.html">Contents</a></li>
<li><a href="/documentation/latest/developer/Tools.html">Setting up the development tools</a></li>
<li><a href="/documentation/latest/developer/Compiling.html">Compiling the Source Code</a></li>
<li><a href="/documentation/latest/developer/Initializing.html">Initializing OpenBUGS</a></li>
<li><a href="/documentation/latest/developer/WritingBUGSExtensions.html">Writing OpenBUGS extensions</a></li>
<li><a href="/documentation/latest/developer/SamplingAlgorithms.html">Sampling algorithms</a></li>
<li><a href="/documentation/latest/developer/BlueDiamonds.html">Blue diamonds</a></li>
<li><a href="/documentation/latest/developer/SoftDocu.html">Bugs subsystems</a></li>
<li><a href="/documentation/latest/developer/Distributing.html">Distributing OpenBUGS</a></li>
</ul>
</li>
<!--<li><a href="/documentation/latest/References.html">References</a>-->
</ul>
</div>
</div></div><div class="col-12 col-md-9 col-lg-7 col-xl-6">


<p>
<h4>Advanced use of the BUGS Language</h4><h5 id="GenericDistribution">Generic sampling distribution</h5>Suppose we wish to use a sampling distribution that is not included in the standard distributions (see the list of <a href="/documentation/latest/Distributions.html"></a><a href="/documentation/latest/Distributions.html">Distributions</a>), in which an observation <em>x</em>[<em>i</em>] contributes a likelihood term <em>L</em>[<em>i</em>] (a function of <em>x</em>[<em>i</em>]). We may use the 'loglik' distribution <code>dloglik</code>, for a dummy observed variable:<br/><br/><code>   dummy[i] &lt;- 0<br/>   dummy[i] ~ dloglik(logLike[i])</code><br/>   <br/>where <code>logLike[i]</code> is the contribution to the log-likelihood for the i<sup>th</sup> observation. The <code>dloglik</code> function implements the 'zero poisson' method utilized in WinBUGS.<br/>   <br/>This is illustrated in the example below in which a normal likelihood is constructed and the results are compared to the standard formulation.<br/><br/><br/><code>   model {<br/>      for (i in 1:7) {<br/>         dummy[i] &lt;- 0<br/>         # likelihood is exp(logLike[i])<br/>         dummy[i] ~ dloglik(logLike[i])<br/>         # log(likelihood)<br/>         logLike[i] &lt;- -log(sigma) - 0.5 * pow((x[i] - mu) / sigma, 2)         <br/>      }<br/>      mu ~ dunif(-10, 10)<br/>      sigma ~ dunif(0, 10)<br/>   }</code><br/><br/><strong>or</strong><br/><br/><code>   model {<br/>      #      check using normal distribution<br/>      for (i in 1:7) {<br/>         x[i] ~ dnorm(mu, prec)<br/>      }<br/>      prec &lt;- 1 / (sigma * sigma)<br/>      mu ~ dunif(-10, 10)<br/>      sigma ~ dunif(0, 10)<br/>   }<br/></code><br/><font color="#0000FF" face="Arial" size="3">Data:</font>list(x = c(-1, -0.3, 0.1, 0.2, 0.7, 1.2, 1.7))<br/><font color="#0000FF" face="Arial" size="3">Initial values:</font>list(sigma = 1, mu = 0)<br/><br/><font color="#0000FF" face="Arial" size="3">Results:</font><br/><font color="#000100" face="Arial" size="2"><strong>      mean   sd   MC_error   val2.5pc   median   val97.5pc   start   sample<br/></strong></font><font color="#000100" face="Arial" size="2">   mu   0.3763   0.447   0.01203   -0.5436   0.3789   1.302   1001   9000<br/>   sigma   1.145   0.4368   0.01743   0.6113   1.047   2.263   1001   9000<br/><br/>or<br/><br/></font><font color="#000100" face="Arial" size="2"><strong>      mean   sd   MC_error   val2.5pc   median   val97.5pc   start   sample<br/></strong></font><font color="#000100" face="Arial" size="2">   mu   0.3642   0.494   0.004657   -0.6213   0.3663   1.342   1001   9000<br/>   sigma   1.208   0.5572   0.01174   0.6299   1.083   2.486   1001   9000<br/><br/></font><br/><h5 id="SpecifyingANewPriorDistribution">Specifying a new prior distribution</h5>For a parameter <code>theta</code>, if we want to use a prior distribution not included in the standard distributions (see the list of <a href="/documentation/latest/Distributions.html"></a><a href="/documentation/latest/Distributions.html">Distributions</a>), then we can use the '<code>dloglik</code>' distribution (see above) for the prior specification. A single dummy observation contributes the appropriate term to the likelihood for <code>theta</code>; and when it is combined with a 'flat' prior for <code>theta</code>, the correct distribution results:<br/><br/><code>         theta ~ dflat()<br/>         dummy ~ dloglik(logLike)<br/>         logLike &lt;- log(desired prior for theta)<br/></code><br/>This is illustrated by the below example in which a normal prior is constructed and the results are compared to the standard formulation. It is important to note that this method will cause the theta variable to be sampled using the metropolis algorithm and may lead to poor convergence and a high Monte Carlo error<br/><br/><code>model {<br/>for (i in 1:7) {<br/>x[i] ~ dnorm(mu, prec)<br/>}<br/>dummy &lt;- 0<br/>dummy ~ dloglik(phi) # likelihood is exp(phi)<br/>phi &lt;- -0.5 * pow(mu, 2) # log(N(0, 1))<br/>mu ~ dflat() # 'flat' prior<br/>prec &lt;- 1 / (sigma * sigma)<br/>sigma ~ dunif(0, 10)<br/>}</code><br/><br/>or<br/><br/><code>model {<br/>for (i in 1:7) {<br/>x[i] ~ dnorm(mu, prec)<br/>}<br/>mu ~ dnorm(0, 1) # 'known' normal prior<br/>prec &lt;- 1 / (sigma * sigma)<br/>sigma ~ dunif(0, 10)<br/>}<br/><br/></code><br/><font color="#0000FF" face="Arial" size="3">Data:</font>list(x = c(-1, -0.3, 0.1, 0.2, 0.7, 1.2, 1.7))<br/>   <br/><font color="#0000FF" face="Arial" size="3">Initial values:</font>list(sigma = 1, mu = 0)<br/><br/><font color="#0000FF" face="Arial" size="3">Results:</font><br/><font color="#000100" face="Arial" size="2"><strong>      mean   sd   MC_error   val2.5pc   median   val97.5pc   start   sample<br/></strong></font><font color="#000100" face="Arial" size="2">   mu   0.2859   0.4108   0.008778   -0.5574   0.3002   1.08   1001   9000<br/>   sigma   1.162   0.4721   0.008885   0.6178   1.05   2.359   1001   9000<br/>   <br/>   or<br/>   <br/></font><font color="#000100" face="Arial" size="2"><strong>      mean   sd   MC_error   val2.5pc   median   val97.5pc   start   sample<br/></strong></font><font color="#000100" face="Arial" size="2">   mu   0.2973   0.4043   0.003525   -0.5499   0.3112   1.07   1   10000<br/>   sigma   1.166   0.4785   0.007654   0.6234   1.059   2.315   1   10000<br/><br/></font><br/><h5 id="UsingPDAndDIC">Using pD and DIC</h5>Here we make a number of observations regarding the use of DIC and pD - for a full discussion see <a href="/documentation/latest/References.html#DICPaper"></a><a href="/documentation/latest/References.html#DICPaper">Spiegelhalter et al (2002)</a>:<br/><br/>1) DIC is intended as a generalisation of Akaike's Information Criterion (AIC). For non-hierarchical models, pD should be approximately the true number of parameters.<br/><br/>2) Slightly different values of Dhat (and hence pD and DIC) can be obtained depending on the parameterisation used for the prior distribution. For example, consider the precision <em>tau</em> (1 / variance) of a normal distribution. The two priors<br/><br/>      <code>tau ~ dgamma(0.001, 0.001)</code><br/><br/>and<br/><br/>      <code>log.tau ~ dunif(-10, 10); log(tau) &lt;- log.tau</code><br/><br/>are essentially identical but will give slightly different results for Dhat. The first prior distribution has stochastic parent <em>tau</em> and hence the posterior mean of <em>tau</em> is substituted in Dhat, while in the second parameterisation the stochastic parent is <em>log.tau</em> and hence the posterior mean of log(<em>tau</em>) is substituted in Dhat.<br/><br/>3) For sampling distributions that are log-concave in their stochastic parents, pD is guaranteed to be positive (provided the simulation has converged). However, we have obtained negative pD's in the following situations:i) with non-log-concave likelihoods (e.g. Student-t distributions) when there is substantial conflict between prior and data;<br/>ii) when the posterior distribution for a parameter is symmetric and bimodal, and so the posterior mean is a very poor summary statistic and gives a very large deviance.4) No MC error is available on the DIC. MC error on Dbar can be obtained by monitoring deviance and is generally quite small. The primary concern is to ensure convergence of Dbar - it is therefore worthwhile checking the stability of Dbar over a long chain.<br/><br/>5) The minimum DIC estimates the model that will make the best short-term predictions, in the same spirit as Akaike's criterion. However, if the difference in DIC is, say, less than 5, and the models make very different inferences, then it could be misleading just to report the model with the lowest DIC.<br/><br/>6) DICs are comparable only over models with exactly the same observed data, but there is no need for them to be nested.<br/><br/>7) DIC differs from Bayes factors and BIC in both form and aims.<br/><br/>8) Caution is advisable in the use of DIC until more experience has been gained. <strong>It is important to note that the calculation of DIC will be disallowed for certain models. </strong><br/><br/><h5 id="MixturesOfModelsOfDifferentComplexity">Mixtures of models of different complexity</h5>Suppose we assume that each observation, or group of observations, is from one of a set of distributions, where the members of the set have different complexity. For example, we may think data for each person's growth curve comes from either a linear or quadratic line. We might think we would require 'reversible jump' techniques, but this is not the case as we are really only considering a single mixture model as a sampling distribution. Thus standard methods for setting up mixture distributions can be adopted, but with components having different numbers of parameters.<br/><br/>The below example illustrates how this is handled in <em>BUGS</em>, using a set of simulated data.<br/><br/>Suppose that for each i: x[i] ~ N(mu, 1) with probability p; and x[i] ~ N(0, 1) otherwise (i.e. with probability 1 - p). We generate 100 observations with p = 0.4 and mu = 3 as follows. We forward sample <em>once</em> from the model below by compiling the code and then using the 'gen inits' facility. The simulated data can then be obtained by selecting <em>Save State</em> from the <em>Model</em> menu.<br/><br/><code>model {<br/>mu &lt;- 3<br/>p &lt;- 0.4<br/>m[1] &lt;- 0<br/>m[2] &lt;- mu<br/>for (i in 1 : 100) {<br/>group[i] ~ dbern(p)<br/>index[i] &lt;- group[i] + 1<br/>y[i] ~ dnorm(m[index[i]], 1)<br/>}<br/>}<br/></code><br/>We may observe the underlying mixture distribution by monitoring any one of the y[i]'s over a number of additional sampling cycles. For example, the following kernel density plot was obtained after monitoring y[1] for 1000 iterations:<br/><br/><font color="#000100" face="Arial" size="2"><br/>         <img alt="[tricks1]" src="tricks1.bmp"/></font><br/>To analyse the simulated data we use the following code:<br/><br/><code>model {<br/>mu ~ dunif(-5, 5)<br/>p ~ dunif(0, 1)<br/>m[1] &lt;- 0<br/>m[2] &lt;- mu<br/>for (i in 1:100) {<br/>group[i] ~ dbern(p)<br/>index[i] &lt;- group[i] + 1<br/>y[i] ~ dnorm(m[index[i]], 1)<br/>}<br/>}</code><br/><br/>After 101000 iterations (with a burn-in of 1000) we have good agreement with the 'true' values:<br/><br/><font color="#000100" face="Arial" size="2"><strong>      mean   sd   MC_error   val2.5pc   median   val97.5pc   start   sample<br/></strong></font><font color="#000100" face="Arial" size="2">   mu   3.058   0.2071   0.001288   2.655   3.056   3.472   1001   100000<br/>   p   0.4243   0.05985   3.718E-4   0.3098   0.4232   0.5438   1001   100000</font><br/><br/><font color="#0000FF" face="Arial" size="3">Initial values</font>:<br/>list(mu = 0, p = 0.5)<br/><br/><font color="#0000FF" face="Arial" size="3">Simulated data:</font><br/><font color="#000000" face="Arial" size="2">list(<br/>y = c(<br/>2.401893189187883,0.2400077667887621,-0.1556589480882761,-0.8457182007723154,0.37008097263224,<br/>3.586009960655263,1.598955590680827,3.826518138558907,-0.9630895329522409,0.6951468806412424,<br/>3.129328672725175,0.01025316168135796,0.4887298480200992,0.3865632519840305,-0.2697534502300845,<br/>-1.18891944058751,4.654771935717583,0.8063807170988319,-0.9867060769784521,0.9154433557950319,<br/>-0.5419217214549653,3.358942981432516,3.33734145389337,3.3960739633218,2.038185222382867,<br/>5.241414085016386,3.362823353717864,-0.6013483154102028,0.441480491316843,2.96228336554288,<br/>-2.278054802181326,1.446861613005477,1.49864667127073,2.819410923921955,3.668112206659865,<br/>0.8253991892717565,1.117718710956935,3.976040128045549,1.261678474198661,-0.03343173803015926,<br/>-0.4908566523519207,0.3532664087054739,-1.679362022373703,-1.555760053262808,1.213022071911081,<br/>3.421072023150202,2.523431239569371,-1.218844344999495,-0.270208787763775,-0.1217560919494103,<br/>2.033835091596821,0.4654798734609423,-0.7231540561359688,2.146640407714382,4.286633169106659,<br/>1.445348149793759,0.180718235361594,1.527791426438174,-1.010060680847808,1.969758236040937,<br/>-0.3553936244225268,1.465488166547136,3.32669874753109,1.061348836020805,2.31746192198435,<br/>0.9564080472865206,1.877477903911581,-0.6964242592539615,5.695159167887606,2.807268123194206,<br/>-1.69815612298043,0.1881330355284009,0.04018232765300667,2.272096174325846,0.9694913345710192,<br/>3.979152197443702,-0.7028546956095989,1.371423010966528,1.646618045628321,0.1919331499516834,<br/>3.587928853903195,0.1219688057614579,2.570950727333546,1.888563572434331,1.10249950266553,<br/>4.119103539377994,3.824513529111282,1.509248826260637,1.156700011660602,-0.0343697724869282,<br/>3.816521442901518,0.0675520018218364,-0.002197033376085265,2.994147650487649,1.856351699226103,<br/>3.539838516568062,0.5861211569557628,4.343058450523658,0.1760647082096519,3.678908111531086))<br/></font><br/>Naturally, the standard warnings about mixture distributions apply, in that convergence may be poor and careful parameterisation may be necessary to avoid some of the components becoming empty.<br/><br/><h5 id="WhereTheSizeOfASetIsARandomQuantity">Where the size of a set is a random quantity</h5>Suppose the size of a set is a random quantity: this naturally occurs in 'changepoint' problems where observations up to an unknown changepoint K come from one model, and after K come from another. Note that we <strong>cannot</strong> use the construction<br/><br/><code>for (i in 1:K) {<br/>y[i] ~ model 1<br/>}<br/>for (i in (K + 1):N) {<br/>y[i] ~ model 2<br/>}</code><br/>since the index for a loop cannot be a random quantity. Instead we can use the step function to set up an indicator as to which set each observation belongs to:<br/><br/><code>for (i in 1:N) {<br/># will be 1 for all i &lt;= K, 2 otherwise<br/>ind[i] &lt;- 1 + step(i - K - 0.01) <br/>y[i] ~ model ind[i]<br/>}</code><br/><br/>This is illustrated by the problem of adding up terms in a series of unknown length.<br/><br/>Suppose we want to find the distribution of the sum of the first K integers, where K is a random quantity. We shall assume K has a uniform distribution on 1 to 10. <br/><br/><code>model {<br/>for (i in 1:10) {<br/>p[i] &lt;- 1 / 10 # set up prior for K<br/>x[i] &lt;- i # set up array of integers<br/>}<br/>K ~ dcat(p[]) # sample K from its prior<br/>for (i in 1:10) {<br/># determine which of the x[i]'s are to be summed<br/>xtosum[i] &lt;- x[i] * step(K - i + 0.01)<br/>}<br/>s &lt;- sum(xtosum[])<br/>}</code><br/><br/><font color="#000100" face="Arial" size="2"><br/>      <img alt="[tricks2]" src="tricks2.bmp"/></font><br/><h5 id="AssessingSensitivityToPriorAssumptions">Assessing sensitivity to prior assumptions</h5>One way to do this is to repeat the analysis under different prior assumptions, but within the same simulation in order to aid direct comparison of results. Assuming the consequences of <em>K</em> prior distributions are to be compared:<br/><br/>a) replicate the dataset <em>K</em> times within the model code;<br/><br/>b) set up a loop to repeat the analysis for each prior, holding results in arrays;<br/><br/>c) compare results using the <a href="/documentation/latest/InferenceMenu.html#Compare">'compare'</a> facility.<br/>The example <a href="/examples/latest/Magnesium.html"></a><a href="/examples/latest/Magnesium.html">prior-sensitivity</a> explores six different suggestions for priors on the random-effects variance in a meta-analysis.<br/><br/><br/><h5 id="ModellingUnknownDenominators">Modelling unknown denominators</h5>Suppose we have an unknown Binomial denominator for which we wish to express a prior distribution. It can be given a Poisson prior but this makes it difficult to express a reasonably uniform distribution. Alternatively a continuous distribution could be specified and then the 'round' function used. For example, suppose we are told that a fair coin has come up heads 10 times - how many times has it been tossed?<br/><br/><code>model {<br/>r &lt;- 10<br/>p &lt;- 0.5<br/>r ~ dbin(p, n)<br/>n.cont ~ dunif(1, 100)<br/>n &lt;- round(n.cont)<br/>}</code><br/><br/><strong>   node   mean   sd   MC error   2.5%   median   97.5%   start   sample<br/></strong>   n   21.08   4.794   0.07906   13.0   21.0   32.0   1001   5000<br/>   n.cont   21.08   4.804   0.07932   13.31   20.6   32.0   1001   5000<br/><br/><br/>Assuming a uniform prior for the number of tosses, we can be 95% sure that the coin has been tossed between 13 and 32 times. A discrete prior on the integers could also have been used in this context.<br/><br/><h5 id="HandlingUnbalancedDatasets">Handling unbalanced datasets</h5>Suppose we observe the following data on three individuals:<br/><br/>      Person 1: 13.2<br/>      Person 2: 12.3 , 14.1<br/>      Person 3: 11.0, 9.7, 10.3, 9.6<br/>   <br/>There are three different ways of entering such 'ragged' data into WinBUGS:<br/><br/><strong>1. Fill-to-rectangular:</strong> Here the data is 'padded out' by explicitly including the missing data, i.e.<br/><br/><code>      y[,1]   y[,2]   y[,3]   y[,4]<br/>      13.2   NA   NA   NA<br/>      12.3   14.1   NA   NA<br/>      11.0   9.7   10.3   9.6<br/>      END</code><br/><br/>or <code>list(y = structure(.Data = c(13.2, NA, NA, NA, 12.3, 14.1, NA, NA, 11.0, 9.7, 10.3, 9.6), .Dim = c(3, 4))</code>.<br/><br/>A model such as <code>y[i, j] ~ dnorm(mu[i], 1)</code> can then be fitted. This approach is inefficient unless one explicitly wishes to estimate the missing data.<br/><br/><strong>2. Nested indexing:</strong> Here the data are stored in a single array and the associated person is recorded as a factor, i.e.<br/><code>      y[]   person[]<br/>      13.2   1<br/>      12.3   2<br/>      14.1   2<br/>      11.0   3<br/>      9.7   3<br/>      10.3   3<br/>      9.6   3<br/>      END</code><br/><br/>or <code>list(y = c(13.2, 12.3, 14.1, 11.0, 9.7, 10.3, 9.6), person = c(1, 2, 2, 3, 3, 3, 3))</code>.<br/><br/>A model such as <code>y[k] ~ dnorm(mu[person[k]], 1)</code> can then be fitted. This seems an efficient and clear way to handle the problem.<br/><br/><strong>3. Offset: </strong>Here an 'offset' array holds the position in the data array at which each person's data starts. For example, the data might be<br/><br/><code>list(y = c(13.2, 12.3, 14.1, 11.0, 9.7, 10.3, 9.6), offset = c(1, 2, 4, 8))<br/></code><br/>and lead to a model containing the code<br/><br/><code>      for (k in offset[i]:(offset[i + 1] - 1)) {<br/>         y[k] ~ dnorm(mu[i], 1)<br/>      }<br/></code><br/>The danger with this method is that it relies on getting the offsets correct and they are difficult to check. <br/><br/>The three methods are illustrated in the small example below.<br/><br/><strong>Fill-to-rectangular:<br/></strong><code>      model {<br/>         for (i in 1:3) {<br/>            mu[i] ~ dunif(0, 100)<br/>            for (j in 1:4) {<br/>               y[i, j] ~ dnorm(mu[i], 1)<br/>            }<br/>         }<br/>      }<br/></code><code>      y[, 1]   y[, 2]   y[, 3]   y[, 4]<br/>      13.2   NA   NA   NA<br/>      12.3   14.1   NA   NA<br/>      11.0   9.7   10.3   9.6<br/>      END<br/></code><code>list(y = structure(.Data = c(13.2, NA, NA, NA, 12.3, 14.1, NA, NA, 11.0, 9.7, 10.3, 9.6), .Dim = c(3, 4))</code><strong>Nested indexing:<br/></strong><code>      model <br/>      {<br/>         for (i in 1:3) {<br/>            mu[i] ~ dunif(0, 100)<br/>         }<br/>         for (k in 1:7) {<br/>            y[k] ~ dnorm(mu[person[k]], 1)<br/>         }<br/>      }<br/><br/>      y[]   person[]<br/>      13.2   1<br/>      12.3   2<br/>      14.1   2<br/>      11.0   3<br/>      9.7   3<br/>      10.3   3<br/>      9.6   3<br/>      END<br/><br/>list(y = c(13.2, 12.3, 14.1, 11.0, 9.7, 10.3, 9.6), person = c(1, 2, 2, 3, 3, 3, 3))<br/></code><br/><strong>Offset:<br/><br/></strong><code>      model {<br/>         for (i in 1:3) {<br/>            mu[i] ~ dunif(0, 100)<br/>            for (k in offset[i]:(offset[i + 1] - 1)) {<br/>               y[k] ~ dnorm(mu[i], 1)<br/>            }<br/>         }<br/>      }<br/><br/>      y[]<br/>      13.2<br/>      12.3<br/>      14.1<br/>      11.0<br/>      9.7<br/>      10.3<br/>      9.6<br/>      END<br/><br/>      offset[]<br/>      1<br/>      2<br/>      4<br/>      8<br/>      END<br/><br/>list(y = c(13.2, 12.3, 14.1, 11.0, 9.7, 10.3, 9.6), offset = c(1, 2, 4, 8))</code><h5 id="UseOfTheCutFunction">Use of the "cut" function</h5>Suppose we observe some data that we do not wish to contribute to the parameter estimation and yet we wish to consider as part of the model. This might happen, for example:<br/><br/>a) when we wish to make predictions on some individuals on whom we have observed some partial data that we do not wish to use for parameter estimation;<br/>b) when we want to use data to learn about some parameters and not others;<br/>c) when we want evidence from one part of a model to form a prior distribution for a second part of the model, but we do not want 'feedback' from this second part.<br/><br/>The "<code>cut</code>" function forms a kind of 'valve' in the graph: prior information is allowed to flow 'downwards' through the cut, but likelihood information is prevented from flowing upwards.<br/><br/>For example, the following code leaves the distribution for theta unchanged by the observation <code>y</code>.<br/><br/><code>      model <br/>      {<br/>         y &lt;- 2<br/>         y ~ dnorm(theta.cut, 1)<br/>         theta.cut &lt;- cut(theta)<br/>         theta ~ dnorm(0, 1)<br/>      }<br/></code><br/>

</p>


</div><div class="d-none d-lg-block col-lg-3 col-xl-3" id="onthispage"><div class="sticky-top stickymenu"><h5>On this page</h5><ul><li><a href="#GenericDistribution">Generic sampling distribution</a></li><li><a href="#SpecifyingANewPriorDistribution">Specifying a new prior distribution</a></li><li><a href="#UsingPDAndDIC">Using pD and DIC</a></li><li><a href="#MixturesOfModelsOfDifferentComplexity">Mixtures of models of different complexity</a></li><li><a href="#WhereTheSizeOfASetIsARandomQuantity">Where the size of a set is a random quantity</a></li><li><a href="#AssessingSensitivityToPriorAssumptions">Assessing sensitivity to prior assumptions</a></li><li><a href="#ModellingUnknownDenominators">Modelling unknown denominators</a></li><li><a href="#HandlingUnbalancedDatasets">Handling unbalanced datasets</a></li><li><a href="#UseOfTheCutFunction">Use of the "cut" function</a></li></ul></div></div></div></div></body></html>